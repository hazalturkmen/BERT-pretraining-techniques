# A Pipeline Of Simultaneous pretraining with amplified vocabulary Bert On Google TPU
A pipeline of pertaining Bert on your own dataset using google TPU to implement Simultaneous pre-training with amplified vocabulary approach [1] 


# Introduction
Pretrained language models elevated with in-domain corpora show impressive results in biomedicine and clinical NLP tasks in English. However, there is minimal work in low resource languages.This repo introduce Simultaneous pre-training with amplified vocabulary approach [1] to enhance BERT models which are developed from low resource languages. Unlike implementation of proposed work [1], engineering proceses redesigned for TPU devices.




#  Cite
The study will be published as a publication...


#  Bibtex

# References
[1]Wada, S., Takeda, T., Manabe, S., Konishi, S., Kamohara, J., & Matsumura, Y. (2020). Pre-training technique to localize medical bert and enhance biomedical bert. arXiv preprint arXiv:2005.07202.


# Acknowledgements
We would like to acknowledge the support we received from the TPU Research Cloud(TRC) team in providing access to TPUv3 units.
